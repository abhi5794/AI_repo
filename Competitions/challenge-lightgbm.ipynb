{"cells":[{"metadata":{"trusted":true,"_uuid":"64b7b281ebfefdf94d1f0ff807670709d091c01a"},"cell_type":"code","source":"# Simple lgbm without GSV\n# V3 with GSV\n# V4 with the final values of hyperparams\n# V5 taking the mean of 5 predictions\n# V6 with ridge regression\n# V8 with make_score\n# V9 with helper for GridSearch\n# V10 with Ridge\n# V11 withput ridge\n# V12 simple GS","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%env JOBLIB_TEMP_FOLDER=/tmp # prevent memory issues\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom fastai.structured import *\nfrom fastai.column_data import *\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndata = pd.read_csv('../input/training_data.csv')\nX_test = pd.read_csv('../input/test_data.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49dd57ae667572aa1261f9cf46e92334565cec83"},"cell_type":"code","source":"X_test.drop(X_test.columns[[0]],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56d8d186ee1ada267b8dec91465df94ee89b4b8f"},"cell_type":"code","source":"# Removing highly correlated data\n#data = data.drop(data.columns[[5,6,21,25,30]],axis=1)\n#X_test = X_test.drop(X_test.columns[[6,7,22,26,31]],axis=1)\n\nX = data.iloc[:,1:-1]\ny = data.iloc[:,-1]\n\n#X_test = test\n#y_test = np.zeros(test.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3e50fa4098691997aa0fcfdbf934dbfb7aa800e","scrolled":true},"cell_type":"code","source":"X_train, X_valid , y_train, y_valid = train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"add3912979e2759f166013dfc095c4e3fd38acf7"},"cell_type":"code","source":"lgbm_train = lgb.Dataset(X_train,y_train) # Preparing data for lgb\nlgbm_valid = lgb.Dataset(X_valid,y_valid,reference=lgbm_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a75194984c9113378f696a68494c5e20f79d4707"},"cell_type":"code","source":"params = {'boosting_type': 'gbdt',                     # Lgb parameters\n    'objective': 'regression',\n    'metric': 'mape',\n    'learning_rate': 0.06, # updated after GS\n    'feature_fraction': 0.1, # updated after GS\n    'bagging_fraction': 0.5, # updated after GS\n    'bagging_freq': 5, # updated after GS\n    'max_depth' : 2, # updated after GS\n    'verbose': 0,\n    'num_leaves' : 4 # updates after GS\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be4b43eec5a87834b3cf0c98013347043b138397","_kg_hide-output":false},"cell_type":"code","source":"gbm = lgb.train(\n                params,\n               lgbm_train,\n               valid_sets=lgbm_valid,\n                num_boost_round=2000,\n                early_stopping_rounds=50 # stop if the results do not improve in 10 rounds\n                #verbose_eval=False # verbose = 0\n               )\nprint('The best Iteration is',gbm.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a9bac4a1b35c0a12f661257d3953edd7b4f157b"},"cell_type":"code","source":"GS = False # switch GS on and off","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f08ce5ea746a0276cbb39059c5fdb0e4d599e43e"},"cell_type":"code","source":"def meap_(y_true,y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\nmape = make_scorer(meap_,greater_is_better=False)\n\nif GS == True:\n    grid_params = {'max_depth' : [1,2,100],\n                   'num_leaves' : [2,3,4,5,10,50],\n                    'learning_rate': [0.055,0.06,0.061],\n                    'feature_fraction': [0.1,0.5,0.9],\n                    'bagging_fraction': [0.1,0.5,0.8],\n                    'bagging_freq' : [3,5,7]\n    }\n\n\n    mdl = lgb.LGBMRegressor(boosting_type= params['boosting_type'], # these will be commented when the parameter is in grid_params, uncomment when you get the best results.\n                           objective = params['objective'],\n                            metric = params['metric'],\n                            #num_leaves = params['num_leaves'], \n                            #learning_rate = params['learning_rate'],\n                            #feature_fraction = params['feature_fraction'],\n                            #bagging_fraction = params['bagging_fraction'],\n                            #bagging_freq = params['bagging_freq']\n                           )\n\n    grid = GridSearchCV(mdl,\n                       grid_params,\n                        cv = 5,\n                        scoring = mape\n                       )\n\n    grid.fit(X_train,y_train)\n\n    print(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d945e3235347afba9a3800e2f4b0a0ef6c8993fb"},"cell_type":"code","source":"# creating a model that takes the mean prediction\npred_buff = []\nn_itr = 5\nfor i in range(n_itr):\n    X_train, X_valid , y_train, y_valid = train_test_split(X,y,test_size=0.2,random_state=i) # random state is the number in the iteration\n    lgbm_train = lgb.Dataset(X_train,y_train) # Preparing data for lgb\n    lgbm_valid = lgb.Dataset(X_valid,y_valid,reference=lgbm_train) # Preparing data for lgb\n    \n    gbm_itr = lgb.train(\n                params,\n               lgbm_train,\n               valid_sets=lgbm_valid,\n                num_boost_round=150,\n                early_stopping_rounds=5, # stop if the results do not improve in 5 rounds\n                verbose_eval=False # verbose = 0\n               )\n    \n    y_pred = gbm.predict(X_test,\n                     num_iteration=gbm.best_iteration) # run the number of iterations for the best number\n    pred_buff.append(y_pred)\n#pred = np.mean(pred_buff,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de655644e6bf9dfa9363701853f07c969eb91753"},"cell_type":"code","source":"class EstimatorSelectionHelper:\n\n    def __init__(self, models, params):\n        if not set(models.keys()).issubset(set(params.keys())):\n            missing_params = list(set(models.keys()) - set(params.keys()))\n            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n        self.models = models\n        self.params = params\n        self.keys = models.keys()\n        self.grid_searches = {}\n\n    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n        for key in self.keys:\n            print(\"Running GridSearchCV for %s.\" % key)\n            model = self.models[key]\n            params = self.params[key]\n            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n                              verbose=verbose, scoring=scoring, refit=refit,\n                              return_train_score=True)\n            gs.fit(X,y)\n            self.grid_searches[key] = gs    \n    def score_summary(self, sort_by='mean_score'):\n        def row(key, scores, params):\n            d = {\n                 'estimator': key,\n                 'min_score': min(scores),\n                 'max_score': max(scores),\n                 'mean_score': np.mean(scores),\n                 'std_score': np.std(scores),\n            }\n            return pd.Series({**params,**d})\n\n        rows = []\n        for k in self.grid_searches:\n            print(k)\n            params = self.grid_searches[k].cv_results_['params']\n            scores = []\n            for i in range(self.grid_searches[k].cv):\n                key = \"split{}_test_score\".format(i)\n                r = self.grid_searches[k].cv_results_[key]        \n                scores.append(r.reshape(len(params),1))\n\n            all_scores = np.hstack(scores)\n            for p, s in zip(params,all_scores):\n                rows.append((row(k, s, p)))\n\n        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n\n        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n        columns = columns + [c for c in df.columns if c not in columns]\n\n        return df[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2d102dcc46f74439d62119446ef5211d3629aa6"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c9bc4a83ad7849edf9389c4d1e9b5848de1578e"},"cell_type":"code","source":"GS = True\nif GS == True:\n    models1 = {\n        'ExtraTreesRegressor': ExtraTreesRegressor(),\n        'RandomForestRegressor': RandomForestRegressor(),\n        'AdaBoostRegressor': AdaBoostRegressor(),\n        'GradientBoostingRegressor': GradientBoostingRegressor(),\n#        'SVR': SVR()\n    }\n\n    params1 = {\n        'ExtraTreesRegressor': { 'n_estimators': [16, 32] },\n        'RandomForestRegressor': { 'n_estimators': [16, 32] },\n        'AdaBoostRegressor': { 'n_estimators': [16, 32] },\n        'GradientBoostingRegressor': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n #       'SVR': [\n #           {'kernel': ['linear'], 'C': [1, 10]},\n #           {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n #       ]\n    }\n\n    helper1 = EstimatorSelectionHelper(models1, params1)\n    helper1.fit(X_train, y_train, scoring='neg_mean_absolute_error', n_jobs=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b68c79471d3083d400ec8f12a95da0001bda3884"},"cell_type":"code","source":"if GS == True:\n    from sklearn.linear_model import Ridge\n    par = {\n        #'alpha' : [7000000,8000000]\n        'alpha' : list(np.arange(7547000,7548000,10))\n    }\n    mdl = Ridge()\n    gs = GridSearchCV(mdl,\n                     par,\n                     cv = 2,\n                     scoring=mape)\n    gs.fit(X_train,y_train)\n    print(gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9397731bda16ac7cfa348753646cfb4e508274f"},"cell_type":"code","source":"Ridge = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"617f5bfcb9ad2f23185b65de28c2115a2df08b40"},"cell_type":"code","source":"if Ridge == True:\n    ridge = Ridge(alpha=7547000)\n    ridge.fit(X_train,y_train)\n\n    pred_buff.append(ridge.predict(X_test))\n    pred = np.mean(pred_buff,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a5747c9b21819ec7cd344af01203f69050cf3de"},"cell_type":"code","source":"pred = np.mean(pred_buff,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4be48e210f6df7df1e065377fa84a081f318320c"},"cell_type":"code","source":"sub['shares'] = pred\n\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"434e6157437f6a4a811f1a148d503185a4902109"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}