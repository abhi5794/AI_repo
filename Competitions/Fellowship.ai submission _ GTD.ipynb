{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62ce1140a09fb9c91c5e02aaf578d8ff8b365d4a"
      },
      "cell_type": "code",
      "source": "# SVC try \n# OneHotEncoding\n# GridSearch and class_weights\n# grid search for the best kernel\n# limiting data to 10,000\n# run with the results from grid search\n# using best results {'C': 1.0, 'degree': 3.0, 'gamma': 4.0}\n# Reducing the sample size to 50,000 entries for Grid Search\n# accuracy round 4\n# final",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%env JOBLIB_TEMP_FOLDER=/tmp\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split , GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom fastai.structured import *\nfrom fastai.column_data import *\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\n\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the data\ndata = pd.read_csv('../input/globalterrorismdb_0718dist.csv',engine = 'python')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f51702bd69be6838d62b7d9272b53621fcf0bc5"
      },
      "cell_type": "code",
      "source": "# There appears to be a lot of string and binary data\ndata.head(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ac69f84bce9d684c0d0ba30fb561eed659ca769"
      },
      "cell_type": "code",
      "source": "# So there are close to 2 Lakh entries and 135 features\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a27760322dee17315cca3edf87706bb7741397e6"
      },
      "cell_type": "markdown",
      "source": "# List of Steps\n1. Pre Processing\n2. Visualizing \n3. Removing outliers\n4. Validation split\n5. Training the model\n6. Making the final prediction"
    },
    {
      "metadata": {
        "_uuid": "574f40880dacb65efb69c8741fb833ef530020db"
      },
      "cell_type": "markdown",
      "source": "# Pre Processing "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3458b546f370c48a5400d9b3ece15c1c887ccc13"
      },
      "cell_type": "code",
      "source": "# There are a lot of null values which will be eliminated.\nprint(data.isnull().sum().to_frame().sort_values(0,ascending=False))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1ebf9e1b687e872ada71a29df4cdfb297959768"
      },
      "cell_type": "code",
      "source": "# Get the count of attacks from that event\ndata['count'] = data['eventid'].astype(str).str.slice(-2,).map(lambda x : int(x))\n\n# Standardizing the year\ndata.iyear = data.iyear.apply(lambda x: 2018-x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d45ce423c3f7805ac2ca0bfee0e054cf5bd16e0"
      },
      "cell_type": "code",
      "source": "# Categorical variable list : Nomial - 0 or 1 answer\ncat_no = ['extended','crit1','crit2','crit3','doubtterr','multiple','country','region','specificity','attacktype1','success',\n          'suicide','weaptype1','weapsubtype1','targtype1','targsubtype1','natlty1','guncertain1',\n         'claimed','property','ishostkid','ransom','INT_LOG','INT_IDEO']\n\n#Continous variable list\n#nperpcap is removed due to sparsity \ncont = ['count', 'iyear','imonth','iday','latitude','longitude','nperps','nkill','nkillus']\n\ntarget = ['gname']\n\nids = ['eventid']\n\n# Converting it into int\n# -9 is replacing NA values as the same was followed when fillig the data\nfor v in cat_no:\n    data[v] = data[v].fillna(-9).astype('int32')\n\n# Replacing the negative categories\ndata[cat_no] = data[cat_no].replace(-9,0).astype('int32')\n\n# Coverting it into continous variables\nfor v in cont:\n    data[v] = data[v].fillna(0).astype('float32')\n\n# Replacing missing values with zero, the unknown values will be replaced with 0\ndata[['nperps','nkill','nkillus']] = data[['nperps','nkill','nkillus']].replace(to_replace=[-99,-9],value=0).astype('float32')\n\n# Consolidating it\ndata = data[ids+cat_no+cont+target]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f975b9eb0aa1346fdfbd36f3bfdd94463ac56f7"
      },
      "cell_type": "code",
      "source": "# Test set split\ntest = data[data['gname']=='Unknown']\ntest['gname'] = -999\n\n# Training set split\ntrain = data[data['gname']!='Unknown']\n\n# Label encoding gname\nle = LabelEncoder()\nle.fit(train['gname'])\ntrain['gname'] = le.transform(train['gname'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1acb98ac0a0821813b5fd01f469c816078dc87d8"
      },
      "cell_type": "code",
      "source": "train = pd.concat([train,test],axis=0)\ntrain = train.reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba0796b31d0486c71d63c7d80381a0208dc6d107"
      },
      "cell_type": "markdown",
      "source": "# Visualizing "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ff5db369fe04b863505fbb6f53850670067c245"
      },
      "cell_type": "code",
      "source": "# Let's look at the distribution \n# The class distribution is not even and most of the data is missing\nsns.distplot(train.gname)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37d6fa925f226a2618e28e0a5a6d87b549b6e9fa"
      },
      "cell_type": "code",
      "source": "# Lets look at the correlation plot\n# We can see that the variables iyear and count have multicollinearity and so does nkill and nkillus.\n# The correlation index is not high so we can keep them.\ncorrmat = train.corr()\nsns.heatmap(corrmat,vmin=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "39abef51d9569591fc2f8ce74fcf140577d2ca53"
      },
      "cell_type": "markdown",
      "source": "## Removing Outliers\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d7ce1e5426d654c11696f3dc29f81e6e775f10b"
      },
      "cell_type": "code",
      "source": "# 10000 nperps is a large value by any stretch of imagination, so we will be dropping it\n# removing entries with nkill > 500 Y& nkillus > 500\ntrain[['nkill','nkillus','nperps']].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de204eea375c7b27af2568c63fbbfd6bdc650ee5"
      },
      "cell_type": "code",
      "source": "train = train[(train.nperps<10000)]\ntest = test[(test.nperps<10000)]\n\ntrain = train[(train.nkill<500)]\ntest = test[(test.nkill<500)]\n\ntrain = train[(train.nkillus<500)]\ntest = test[(test.nkillus<500)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "706c66144b4b5ba370d6bba05d58bc722f06b0d4"
      },
      "cell_type": "code",
      "source": "# Creating a df to get the size\nsize_train = train.groupby('gname').size().to_frame().sort_values(0,ascending=False)\nsize_train.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01ad45ded1f947644011f4a1d332c2be37387c53"
      },
      "cell_type": "code",
      "source": "# Keeping only the values that have incidents below 50\ntrain = train.drop(list(size_train[size_train[0]<50].index),axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "696ba28088913fa19361639ce33d417e96c75393"
      },
      "cell_type": "code",
      "source": "# The data is highly imbalanced, if the compute limit for kaggle was not limited to 6 hrs then Oversampling would have\n# been performed for the minority class\ntrain[train.gname != -999].gname.hist(bins=100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7a34cc3f9a3d5caa8587e9fe7be03acd063461e"
      },
      "cell_type": "markdown",
      "source": "# Validation split"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d005e7992a537508058ede85b7ad8ec54a77b644"
      },
      "cell_type": "code",
      "source": "# Using the proc_df to scales the continous variables, they will be combined again\ntrain, train_y, nas, mapper = proc_df(train, 'gname', do_scale=True,ignore_flds=['eventid','extended','crit1','crit2',\n                                                                                 'crit3','doubtterr','multiple','country','region',\n                                                                                 'specificity','attacktype1','success','suicide',\n                                                                                 'weaptype1','weapsubtype1','targtype1','targsubtype1',\n                                                                                 'natlty1','guncertain1','claimed','property','ishostkid',\n                                                                                 'ransom','INT_LOG','INT_IDEO'])\n\ntrain = train.reset_index(drop=True)\ntrain = pd.concat([train,pd.DataFrame(train_y)],axis=1,join_axes=[train.index])\ntrain=train.rename(index=str,columns={0:'gname'})\ntrain = train.reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "afdccb0c2b64d122bde9d08f23ef997e7dbc700b"
      },
      "cell_type": "code",
      "source": "# Creating a function to oneHotEncode all the categorical variables since there are not ordinal\ndef ohe(train,features):\n    \"\"\"\n    The functions takes the df with the list of arguments to be one hot encoded and returns a df\n    train is the df and features is a list\n    \"\"\"\n    for v in features:\n        df = train[v].values\n        train = train.drop([v],axis=1)\n        oh = OneHotEncoder(sparse=False)\n        df = df.reshape(len(df),1)\n        df = oh.fit_transform(df)\n        df = df[:,1:]\n        train = pd.concat([train,pd.DataFrame(df)],axis=1,sort=False)\n    return train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff0b5bb30026a0d279161080b3388ebc8c6df31c"
      },
      "cell_type": "code",
      "source": "train = ohe(train,['country','region','specificity','attacktype1','weaptype1','weapsubtype1','targtype1','targsubtype1','natlty1'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1806a42e03be3bf4af4d3d6a41284855b05ccf04"
      },
      "cell_type": "code",
      "source": "train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb2a2f69d9872d3dc5313ae355b5005b591b032c"
      },
      "cell_type": "code",
      "source": "# Looking at the final DataFrame\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58ec82ca35c8e3d43cb6fe1e8d6e386094cdecfa"
      },
      "cell_type": "code",
      "source": "# Test set split\ntest = train[train.gname == -999]\n\n# Training set split\ntrain = train[train.gname!= -999]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "346a3792f4e5d1190717dd771c23732559f07095"
      },
      "cell_type": "code",
      "source": "test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5df288278ed25a96d28162b4ea94c3332e95101"
      },
      "cell_type": "code",
      "source": "# Limiting the data so as to keep the compute time low\n# Kaggle kernel has a limit of 6 hrs.\ntrain = train.iloc[:50000,:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a14de17c0e17b2997c499570ab8836eb332d0151"
      },
      "cell_type": "code",
      "source": "# To check if any null values have crept in\ntrain.isnull().sum().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f12d34bda047b341126a0db33ceccf1c9a02b707"
      },
      "cell_type": "code",
      "source": "# Dropping the event ID\ntrain.drop(['eventid'],axis='columns',inplace=True)\n\n# Creating the split for validation\ntrainData, validData = train_test_split(train,test_size=0.3)\n\ntrain_X, train_y, nas = proc_df(trainData, 'gname', do_scale=False)\nvalid_X, valid_y, nas = proc_df(validData, 'gname', do_scale=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9270b5ad2dab174ea7e986c04abb2f3df4138ae2"
      },
      "cell_type": "code",
      "source": "train_X.shape,train_y.shape,valid_X.shape,valid_y.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "837b14f5b1ea6527bbbc2102906f97cbe27c091d"
      },
      "cell_type": "markdown",
      "source": "# Training the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67024bc344f7e296d3a5447914f7d38d9ebfb7bb"
      },
      "cell_type": "code",
      "source": "# The SVC model had the best overall accruaracy score among the models tested\n# Using the best results from GridSearch\nest = SVC(C=1, gamma=4, kernel='poly', degree=3, max_iter=10000, class_weight='balanced')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa48161787d756f7a70e7f930258fb4e4f3d2be1"
      },
      "cell_type": "code",
      "source": "# Fitting the training data\nest.fit(train_X,train_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb27b9015a6047e581e08fd3e455644f606e63c0"
      },
      "cell_type": "code",
      "source": "# Making the prediction\ny_pred = est.predict(valid_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "549f1b7b6adbb6ee6f02bbc5a25e293c2268d764"
      },
      "cell_type": "code",
      "source": "# Checking the accruracy and F1 score\n# The F1 score is considered as it paints a better picture when compared to accuracy as accuracy can be high for a highly imbalanced\n# problem like ours.\naccuracy_score(valid_y,y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5653024ea5929ddaafd48787960ac056200b80a9"
      },
      "cell_type": "code",
      "source": "f1_score(valid_y,y_pred,average='macro')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a92f6b22615f8e7a2d932ae58691545b22be57dc"
      },
      "cell_type": "code",
      "source": "# The model was run 4 times to get the average accuracy and f1 score, this is done as the time complexity would be high for performing \n# K-fold cross validation. The limit set for Kaggle notebooks in 6 hrs\n# avg accuracy = 68 % which is the best for among the models tested like lightGBM and KNN & Random forest.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "01488bc6d5cf4f95bd37b1cc2cd6ef0a07f19f34"
      },
      "cell_type": "markdown",
      "source": "# Making the final prediction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77c2e3e062230257c9b2756b2996755f18f6bc76"
      },
      "cell_type": "code",
      "source": "test = test.reset_index(drop=True) # resetting the index of the df\ntest_X = test.drop(['eventid','gname'],axis='columns') # Dropping the event ID\ntest_pred = est.predict(test_X) # making the prediction\ntest_pred = le.inverse_transform(test_pred) # inverse transform\npred = pd.DataFrame(test_pred) # converting the array into a df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3694a1951e3109e186ce22e1b9a5e0daaef6329"
      },
      "cell_type": "code",
      "source": "# The indexes are same so they can be combined.\ntest.index,pred.index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90d846dcae0f1afd961c5a90fbbc1a23b6abcffa"
      },
      "cell_type": "code",
      "source": "final = pd.concat([test.eventid,pred],axis=1,join_axes=[test.index]) # combining the final data\n\nfinal.to_csv('Final_pred.csv') # Exporting it to csv file ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}